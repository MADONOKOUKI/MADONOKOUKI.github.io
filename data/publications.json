[
  {
    "year": 2025,
    "type": "journal",
    "title": "Interactive Texture Segmentation of 3D Scanned Models",
    "authors": "Koki Madono, Takeo Igarashi, Hiroharu Kato, Taisuke Hashimoto, Fabrice Matulic, Tsukasa Takagi, Keita Higuchi",
    "venue": "IEEE Computer Graphics and Applications (also presented CHI 2025 LBW)",
    "links": [
      { "label": "PDF", "url": "https://diglib.eg.org/items/9872e9ad-5498-499f-a10e-94a5f825a68d" }
    ],
    "abstract": "In 3D model scanning, the raw texture of a 3D model often requires segmentation into distinct regions to apply different material properties to each region. Current methods, such as manual segmentation, are labor-intensive, while automatic segmentation techniques lack user control. We propose an interactive tool that combines automatic segmentation with minimal manual intervention, striking an optimal balance between efficiency and control. Following a multiview automatic segmentation process that divides the texture into small subsegments, users cluster the subsegments into segments by drawing simple scribbles in the 3D model view. We show that our approach results in more detailed subsegments compared to automatic segmentation approaches. Furthermore, a user study confirms that our approach improves segmentation accuracy and quality compared to manual segmentation with standard 3D computer graphics software. This research paves the way to more efficient texture segmentation in 3D model scanning.",
    "bibtex": "@ARTICLE{11108266,\n author={Madono, Koki and Igarashi, Takeo and Kato, Hiroharu and Hashimoto, Taisuke and Matulic, Fabrice and Takagi, Tsukasa and Higuchi, Keita},\n journal={ IEEE Computer Graphics and Applications },\n title={{ Interactive Texture Segmentation of 3D Scanned Models Leveraging Multiview Automatic Segmentation }},\n year={2025},\n volume={},\n number={01},\n ISSN={1558-1756},\n pages={1-14},\n keywords={Three-dimensional displays;Image segmentation;Solid modeling;Manuals;Image color analysis;Computational modeling;Brushes;Cameras;Accuracy;Training},\n doi={10.1109/MCG.2025.3595378},\n url = {https://doi.ieeecomputersociety.org/10.1109/MCG.2025.3595378},\n publisher={IEEE Computer Society},\n address={Los Alamitos, CA, USA},\n month=aug}\n",
    "thumb": "images/cga2025.png"
  },
  {
    "year": 2023,
    "type": "journal",
    "title": "Data-Driven Ink Painting Brushstroke Rendering",
    "authors": "Koki Madono, Edgar Simo-Serra",
    "venue": "Computer Graphics Forum (Pacific Graphics)",
    "links": [
      { "label": "PDF", "url": "pdfs/pg2023.pdf" }
    ],
    "abstract": "Although digital painting has advanced much in recent years, there is still a significant divide between physically drawn paintings and purely digitally drawn paintings. These differences arise due to the physical interactions between the brush, ink, and paper, which are hard to emulate in the digital domain. Most ink painting approaches have focused on either using heuristics or physical simulation to attempt to bridge the gap between digital and analog, however, these approaches are still unable to capture the diversity of painting effects, such as ink fading or blotting, found in the real world. In this work, we propose a data-driven approach to generate ink paintings based on a semi-automatically collected high-quality real-world ink painting dataset. We use a multi-camera robot-based setup to automatically create a diversity of ink paintings, which allows for capturing the entire process in high resolution, including capturing detailed brush motions and drawing results. To ensure high-quality capture of the painting process, we calibrate the setup and perform occlusion-aware blending to capture all the strokes in high resolution in a robust and efficient way. Using our new dataset, we propose a recursive deep learning-based model to reproduce the ink paintings stroke by stroke while capturing complex ink painting effects such as bleeding and mixing. Our results corroborate the fidelity of the proposed approach to real hand-drawn ink paintings in comparison with existing approaches. We hope the availability of our dataset will encourage new research on digital realistic ink painting techniques.",
    "bibtex": "@article{10.1111:cgf.14965,\n journal = {Computer Graphics Forum},\n title = {{Data-Driven Ink Painting Brushstroke Rendering}},\n author = {Madono, Koki and Simo-Serra, Edgar},\n year = {2023},\n publisher = {The Eurographics Association and John Wiley & Sons Ltd.},\n ISSN = {1467-8659},\n DOI = {10.1111/cgf.14965}\n}",
    "thumb": "images/pg23_ink.png"
  },
  {
    "year": 2023,
    "type": "conference",
    "title": "Automatic Vector Caricature via Face Parametrization",
    "authors": "Koki Madono, Yannick Hold-Geoffroy, Yijun Li, Daichi Ito, Jose Echevarria, Cameron Smith",
    "venue": "Pacific Graphics, Conference Proceedings",
    "links": [
      { "label": "PDF", "url": "https://diglib.eg.org/items/9872e9ad-5498-499f-a10e-94a5f825a68d" }
    ],
    "abstract": "Automatic caricature generation is a challenging task that aims to emphasize the subject's facial characteristics while preserving identity. We propose Parametric Caricature, a parametric-based caricature generation method that yields vectorized and animatable caricatures. Several hundred parameters encode facial traits predicted from input faces, and part-aware retrieval from an artist-made caricature database guides stylization. The resulting caricatures are visually appealing and suitable as avatars, validated by a user study.",
    "bibtex": "@inproceedings{10.2312:pg.20231271,\n booktitle = {Pacific Graphics Short Papers and Posters},\n editor = {Chaine, Raphaëlle and Deng, Zhigang and Kim, Min H.},\n title = {{Automatic Vector Caricature via Face Parametrization}},\n author = {Madono, Koki and Hold-Geoffroy, Yannick and Li, Yijun and Ito, Daichi and Echevarria, Jose and Smith, Cameron},\n year = {2023},\n publisher = {The Eurographics Association},\n ISBN = {978-3-03868-234-9},\n DOI = {10.2312/pg.20231271}\n}",
    "thumb": "images/pg23_caricature.png"
  },
  {
    "year": 2023,
    "type": "conference",
    "title": "ScrambleMix: A Privacy-Preserving Image Processing for Edge-Cloud Machine Learning",
    "authors": "Koki Madono, Masayuki Tanaka, Masaki Onishi",
    "venue": "Pacific-Rim Symposium on Image and Video Technology (PSIVT)",
    "links": [
      { "label": "PDF",  "url": "https://link.springer.com/chapter/10.1007/978-981-97-0376-0_25" },
      { "label": "CODE", "url": "https://github.com/MADONOKOUKI/psivt23_scramblemix" }
    ],
    "abstract": "This paper proposes ScrambleMix, a novel privacy-preserving image processing for edge-cloud machine learning. ScrambleMix combines image scrambling and AugMix to improve visual information hiding. Specifically, to make two scrambled images from a single input image, each copy of the input image is scrambled using a different key every time. Then, the scrambled images are mixed with a randomly sampled mixing ratio. A self-teaching loss is introduced to improve the classification performance of ScrambleMix. In this study, we first evaluate the visual information hiding quantitatively using Learned Perceptual Image Patch Similarity (LPIPS). Then, the experiments with different settings demonstrate the proposed ScrambleMix outperforms the existing approaches for edge-cloud machine learning in terms of both classification accuracy and visual information hiding.",
    "bibtex": "@inproceedings{10.1007/978-981-97-0376-0_25,\n author = {Madono, Koki and Tanaka, Masayuki and Onishi, Masaki},\n title = {ScrambleMix: A Privacy-Preserving Image Processing for Edge-Cloud Machine Learning},\n year = {2023},\n isbn = {978-981-97-0375-3},\n publisher = {Springer-Verlag},\n address = {Berlin, Heidelberg},\n url = {https://doi.org/10.1007/978-981-97-0376-0_25},\n doi = {10.1007/978-981-97-0376-0_25},\n booktitle = {Image and Video Technology: 11th Pacific-Rim Symposium, PSIVT 2023, Auckland, New Zealand, November 22–24, 2023, Proceedings},\n pages = {326–340},\n numpages = {15},\n keywords = {Image Processing, Cloud-based Machine Learning, Privacy-Preserving Machine Learning},\n location = {Auckland, New Zealand}}",
    "thumb": "images/psivt23.png"
  },
  {
    "year": 2022,
    "type": "conference",
    "title": "Instance-wise Center Loss for Efficient Training of Deep Convolutional Neural Networks",
    "authors": "Koki Madono, Masayuki Tanaka, Masaki Onishi",
    "venue": "IEEE Global Conference on Consumer Electronics (GCCE)",
    "links": [
      { "label": "PDF",  "url": "https://ieeexplore-ieee-org.waseda.idm.oclc.org/document/10014037" },
      { "label": "CODE", "url": "https://github.com/MADONOKOUKI?tab=repositories" }
    ],
    "abstract": "Intermediate feature regularization is a critical technique in machine learning for computer vision applications. We propose an instance-wise center loss regularization with augmented images, enforcing logit consistency among augmentations of the same base image. Experiments show improvements over existing regularization techniques.",
    "bibtex": "@INPROCEEDINGS{10014037,\n author={Madono, Koki and Tanaka, Masayuki and Onishi, Masaki},\n booktitle={2022 IEEE 11th Global Conference on Consumer Electronics (GCCE)},\n title={Instance-wise Center Loss for Efficient Training of Deep Convolutional Neural Networks},\n year={2022},\n volume={},\n number={},\n pages={692-696},\n keywords={Training;Learning systems;Computer vision;Machine learning;Convolutional neural networks;Consumer electronics;Image classification},\n doi={10.1109/GCCE56475.2022.10014037}\n}",
    "thumb": "images/gcce22.png"
  },
  {
    "year": 2021,
    "type": "journal",
    "title": "SIA-GAN: Scrambling Inversion Attack Using Generative Adversarial Network",
    "authors": "Koki Madono, Masayuki Tanaka, Masaki Onishi, Tetsuji Ogawa",
    "venue": "IEEE Access",
    "links": [
      { "label": "PDF",  "url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9537763" },
      { "label": "CODE", "url": "https://github.com/MADONOKOUKI/SIA-GAN" }
    ],
    "abstract": "Intermediate feature regularization is a critical technique in machine learning for computer vision applications. We propose an instance-wise center loss regularization with augmented images, enforcing logit consistency among augmentations of the same base image. Experiments show improvements over existing regularization techniques.",
    "bibtex": "@ARTICLE{9537763,\n author={Madono, Koki and Tanaka, Masayuki and Onishi, Masaki and Ogawa, Tetsuji},\n journal={IEEE Access},\n title={SIA-GAN: Scrambling Inversion Attack Using Generative Adversarial Network},\n year={2021},\n volume={9},\n number={},\n pages={129385-129393},\n keywords={Training;Feature extraction;Visualization;Generators;Generative adversarial networks;Transforms;Machine learning;Artificial intelligence;machine learning;computer vision;visual information hiding;image scrambling},\n doi={10.1109/ACCESS.2021.3112684}}",
    "thumb": "images/access2021.png"
  },
  {
    "year": 2021,
    "type": "conference",
    "title": "Scrambling parameter generation to improve perceptual information hiding",
    "authors": "Koki Madono, Masayuki Tanaka, Masaki Onishi, Tetsuji Ogawa",
    "venue": "Electronic Imaging",
    "links": [
      { "label": "PDF",  "url": "https://library.imaging.org/admin/apis/public/api/ist/website/downloadArticle/ei/33/11/art00007" },
      { "label": "CODE", "url": "https://github.com/MADONOKOUKI" }
    ],
    "abstract": "The present study proposes the method to improve the perceptual information hiding in image scramble approaches. Image scramble approaches have been used to overcome the privacy issues on the cloud-based machine learning approach. The performance of image scramble approaches are depending on the scramble parameters; because it decides the performance of perceptual information hiding. However, in existing image scramble approaches, the performance by scrambling parameters has not been quantitatively evaluated. This may be led to show private information in public. To overcome this issue, a suitable metric is investigated to hide PIH, and then scrambling parameter generation is proposed to combine image scramble approaches. Experimental comparisons using several image quality assessment metrics show that Learned Perceptual Image Patch Similarity (LPIPS) is suitable for PIH. Also, the proposed scrambling parameter generation is experimentally confirmed effective to hide PIH while keeping the classification performance.",
    "bibtex": "@article{madono2021scrambling,\n title={Scrambling parameter generation to improve perceptual information hiding},\n author={Madono, Koki and Tanaka, Masayuki and Onishi, Masaki and Ogawa, Tetsuji},\n journal={Electronic Imaging},\n volume={33},\n pages={1--8},\n year={2021},\n publisher={Society for Imaging Science and Technology}}",
    "thumb": "images/ei21.png"
  }  
]
